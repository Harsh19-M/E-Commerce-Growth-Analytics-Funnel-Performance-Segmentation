# %%
import pandas as pd
from sqlalchemy import create_engine
import os
from sqlalchemy import text

# %%
# CONNECTING to our DB 

DB_USER = "postgres"
DB_PASS = "ThePasswordis19!"
DB_HOST = "localhost"
DB_PORT = 5432
DB_NAME = "E-Commerce_Project"
SCHEMA = "ecom"

# Lets create engine 

engine = create_engine(f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}")


# %%
agg_table_name = "agg_etable"  # Table name in ecom schema

query = f'SELECT * FROM ecom.{agg_table_name}'

agg_df = pd.read_sql(query, engine)


# %%
agg_df.shape  # LOOKS ABOUT RIGHT --> Final output was exactly 1563 rows and the columns looks about right

# %%
agg_df.duplicated().sum() # so no duplicated records 

# %%
agg_df.isnull().sum().sum()

# %%
agg_df["user_id"].nunique()

# %%
agg_df.shape[0]

# %%
agg_df.shape[1]

# %%
agg_df

# %%
# Knowing the data types 
agg_df.dtypes

# %%
agg_df.describe()

# %%
dates = ["signup_date", "start_date", "end_date"]

# %%
for col in dates:
    agg_df[col] = pd.to_datetime(agg_df[col], errors="coerce") # right so the errors="coerce" step makes sure that any invalid entries turn to NaN

# %%
agg_df[["signup_date", "start_date", "end_date"]].dtypes 

# %%
# Check whether the ExpeirmentIDs are correct as in unique or not and whether there are 2 versions of Experiments
agg_df["experiment_id"].nunique()

# %%
agg_df["Experiment Version"].nunique()

# %%
agg_df["Total Sessions"].sum()

# %%
agg_df["Total Sessions"].nunique()

# %%
agg_df.columns

# %%
agg_df.groupby(['experiment_id', 'Experiment Version']).size()

# %%

##---> Exploration Questions for Python Analysis <---##

'''
1. Experiment Exposure & Distribution
    How many users were exposed to each experiment (experiment_id) and version (A/B)?
    Are the experiments roughly balanced in terms of users and sessions?
    Are there any experiments with abnormal session counts or missing exposure?
ANS:
One-liner takeaway for Q1: All experiments are roughly balanced in terms of user exposure and sessions, 
with minor differences across versions that are typical in real-world A/B testing.

Fully answered all parts of Question 1: Experiment Exposure & Distribution:
Users exposed per experiment & version â†’ 
Done (agg_df.groupby(["experiment_id", "Experiment Version"])["user_id"].nunique()) 
Rough balance in users and sessions â†’ 
Done (agg_df.groupby(["Experiment Version"])[["user_id","Total Sessions"]].sum() & checked per experiment) 

Abnormal session counts / missing exposure â†’ Done (agg_df.groupby(["experiment_id", "Experiment Version"])[["Total Sessions"]].sum()) 
One-line finding for Q1:
Experiments are roughly balanced across versions, with unique user counts and total sessions distributed reasonably; 
no missing exposure detected and session counts per experiment fall within expected ranges.

    

2. Conversion Metrics per Experiment
    How do clicks, add-to-cart actions, and completed purchases differ by experiment and version?
    Are there large discrepancies in conversion rates across versions?
    Which experiments show the highest lift in key metrics?

3. User Behavior
    What is the average number of pages viewed per session across experiments and versions?
    Are there patterns in session frequency per user (Total Sessions) that affect conversion?
    Do users who engage more (clicks, pages viewed) convert more often?

4. Customer Segmentation
    Are there patterns in customer profiles (country, city, signup date) that correlate with higher conversion?
    Do certain customer segments respond better to specific experiment variations?
    Are there high-value or discount-sensitive segments (high add-to-cart, low completed purchase ratio)?

5. Temporal Trends
    Are there trends by signup date or session date that suggest seasonal/temporal effects on conversion?
    Do users exposed earlier to experiments behave differently than later users?

6. Event-Level Insights
    Are there any sessions with clicks but no add-to-cart or purchase? What percentage?
    Are there patterns in drop-off (click â†’ add-to-cart â†’ purchase) across experiment variations?

7. Outlier Detection & Data Quality
    Are there extreme values in pages viewed, orders, or order value?
    Are there sessions with inconsistent data (e.g., completed_purchase=1 but no order recorded)?
'''

# %%
'''
1. Experiment Exposure & Distribution
    How many users were exposed to each experiment (experiment_id) and version (A/B)?
    Are the experiments roughly balanced in terms of users and sessions?
    Are there any experiments with abnormal session counts or missing exposure?
'''

# %%
agg_df.groupby(["experiment_id", "Experiment Version"])[["element_tested"]].describe()

# %%
agg_df.groupby(["experiment_id", "Experiment Version"])["user_id"].count()

# %%
# unique users as per the ExperimentID and Version
agg_df.groupby(["experiment_id", "Experiment Version"])["user_id"].nunique()

# %%
agg_df["Total Sessions"].describe()

# %%
# Are the experiments roughly balanced in terms of users and sessions? THESE 4 BELOW DO NOT ANSWER IT PROPERLY (ONES AFTER IT DO)

agg_df.groupby(["user_id", "Total Sessions"])["experiment_id"].count()

# SINCE there are 1435 total outputs - its hard to go over all of em' so this should give us a good idea
agg_df.groupby(["user_id", "Total Sessions"])["experiment_id"].count().describe()


# Balanace in terms of Version 
agg_df.groupby(["experiment_id", "Experiment Version"])[["user_id", "Total Sessions"]].count()

# OR JUST SIMPLE Version wise 
agg_df.groupby(["Experiment Version"])[["user_id","Total Sessions"]].count()


# %%
agg_df["Total completed Purchases"]

# %%
agg_df.groupby(["Experiment Version"])[["user_id","Total Sessions", "Total completed Purchases" > 0]].count()


# %%
agg_df.groupby(["Experiment Version"])[["user_id"]].nunique()

'''
Difference = 26 users (~3.4%) â†’ totally acceptable in real A/B tests

User exposure across A/B variants is well balanced, with 765 users exposed to Version A and 739 to Version B,
indicating no meaningful allocation bias before conversion analysis.
'''

# %%
agg_df.groupby(["Experiment Version"])[["Total Sessions"]].sum()

'''
â€œ Session exposure across experiment variants was well balanced (A: 808 sessions, B: 770 sessions), 
indicating fair traffic distribution and supporting valid comparison of downstream conversion metrics.â€

Version A: 808 total sessions
Version B: 770 total sessions
Difference = 38 sessions
Thatâ€™s roughly a ~2.4% difference, which is very small in A/B testing terms.
'''

# %%
'''Are there any experiments with abnormal session counts or missing exposure? '''

'''An experiment or variant exists, but no users or sessions were actually exposed to it, 
or exposure is unexpectedly low compared to others.'''

agg_df.groupby(["experiment_id", "Experiment Version"])[["Total Sessions"]].sum()
'''Within each experiment, sessions are fairly balanced between versions A and B, 
with minor natural variations (e.g., 118 vs 90 in experiment 1).'''

agg_df.groupby(["experiment_id"])[["Total Sessions"]].sum()
'''Total sessions per experiment range from 180 to 220, 
showing that each experiment has a reasonable number of sessions overall.'''

# %%
agg_df.groupby(["experiment_id", "Experiment Version"])[["user_id"]].nunique()

'''
Shows how many unique users were exposed to each experiment version.
Slight imbalance exists in a few experiments (e.g., experiment 1: A = 115, B = 88).
'''

# %%
agg_df.groupby(["experiment_id"])[["user_id"]].nunique()

'''
Total unique users per experiment across both versions. 
Gives the overall size of each experiment (ranges from 178 â†’ 217 users).
'''

# %%
agg_df.groupby(["Experiment Version"])[["user_id"]].nunique()

'''
Total unique users per A/B version across all experiments. 
Version A = 765 users, Version B = 739 users â†’ roughly balanced.
'''

# %%
'''
One-liner takeaway for Q1: All experiments are roughly balanced in terms of user exposure and sessions, 
with minor differences across versions that are typical in real-world A/B testing.
'''

# %%
'''
2. Conversion Metrics per Experiment
    How do clicks, add-to-cart actions, and completed purchases differ by experiment and version?
    Are there large discrepancies in conversion rates across versions?
    Which experiments show the highest lift in key metrics?

'''

# %%
# How do clicks, add-to-cart actions, and completed purchases differ by experiment and version?
agg_df.groupby(["experiment_id", "Experiment Version"])[["Total clickedProduct", "Total added to cart", "Total completed Purchases"]].sum()

'''
(One sentence **per experiment**, focused on *difference between A vs B*.)
**Overall framing sentence (optional, before bullets):**
> User actions (clicks, add-to-cart, and purchases) vary meaningfully between A and B across experiments, indicating behavioral impact rather 
than uniform performance.
**Per-experiment one-liners:**

* **Experiment 1:** Version A drove more clicks and purchases, while Version B slightly outperformed in add-to-cart actions.
* **Experiment 2:** Version B clearly outperformed Version A across all three metricsâ€”clicks, add-to-cart, and purchases.
* **Experiment 3:** Version A generated more clicks, but Version B converted substantially better with higher completed purchases.
* **Experiment 4:** Version A led in clicks and add-to-cart actions, while Version B produced significantly more purchases.
* **Experiment 5:** Version B showed stronger engagement (more clicks and carts), but Version A resulted in more completed purchases.
* **Experiment 6:** Version A consistently outperformed Version B in clicks and purchases, with equal add-to-cart activity.
* **Experiment 7:** Version A drove more clicks, but Version B converted better with notably higher purchases.
* **Experiment 8:** Version A led in clicks, while Version B achieved higher completed purchases with equal cart additions.

If you want, next we can:
* Convert these into **conversion rates** (CTR, cart rate, purchase rate), or
* Identify **which experiments show the strongest lift** (the real winner slide).
'''

# %%
# Are there large discrepancies in conversion rates across versions?

agg_df.groupby(["experiment_id", "Experiment Version"])[["effect_on_conv"]].mean()

'''

### One-liner finding (what this tells us)

> **There are *no discrepancies* in conversion impact between Version A and B within any experiment â€” 
the average `effect_on_conv` is identical for both versions across all experiments.**
---
BUT BUT BUT DOES this mean A and B Performed identically/same in Reality?

### Why this happened (important, but simple)
This result **does not mean** A and B performed the same in reality.

It means:
* `effect_on_conv` is **already a pre-computed experiment-level metric**
* That value is **constant for all rows belonging to the same experiment**
* When you take a `mean()` by **experiment Ã— version**, youâ€™re averaging the *same number*, so A and B end up identical
---
In short:
> **You grouped a column that does not vary by version â€” so the mean canâ€™t differ.**
---
This naturally leads to the next step:
**Compute conversion rates from raw behavior metrics**
(e.g., purchases Ã· sessions or purchases Ã· users)
---

### How you should phrase this in README / EDA (recommended)
> *The precomputed `effect_on_conv` metric does not vary by experiment version, 
indicating it is stored at the experiment level rather than the variant level. 
Therefore, version-level conversion differences will be calculated directly from user behavior metrics in the next analysis step.*

'''

# %%
'''
Primary behavioral metrics (MOST important)

These vary by user Ã— version, so they can reveal real A/B differences:

Total Sessions â†’ denominator for most rates
Total clickedProduct â†’ click-through behavior
Total added to cart â†’ mid-funnel intent
Total completed Purchases â†’ core conversion metric
Total Orders(Total Order IDs) â†’ order-level confirmation
Total Order_Value â†’ revenue impact
Total Profit â†’ business impact (best for recommendations)

'''

# %%
# Click-through Behaviour 
agg_df["Click-through_Behaviour"] = agg_df["Total clickedProduct"]/agg_df["Total Sessions"]

# %%
agg_df.groupby(["experiment_id", "Experiment Version"])[["Click-through_Behaviour"]].mean()

'''
One-liner findings (use these in README / notes)

Experiment 1 & 2: Version B shows higher click-through rates than A, indicating improved engagement.
Experiment 3, 4, 7, 8: Version A slightly outperforms B, suggesting the change may reduce initial clicks.
Experiment 6: Click-through rates are nearly identical, implying no meaningful impact.
Overall: Click-through performance varies by experiment, reinforcing the need to evaluate 
each UI change independently rather than assuming global uplift.


What this answers (clearly)

âœ” How do clicks differ by experiment and version?
âœ” Are there visible engagement differences pre-testing?
X Does not yet prove significance â†’ thatâ€™s next phase.
'''


# %%
'''
FUNNEL-ANALYSIS (VERSION A | B) BREAKDOWN:

Click-through = interest
Add-to-cart = intent
Purchase = commitment
'''

# %%
# Total added to cart â†’ mid-funnel intent

agg_df["Add_to_cart Rate"] = agg_df["Total added to cart"]/agg_df["Total Sessions"]

# %%
agg_df.groupby(["experiment_id", "Experiment Version"])[["Add_to_cart Rate"]].mean()

'''
One-liner findings (copy-paste ready)
Experiments 1, 2, 5, 6, 7, and 8: Version B shows higher add-to-cart rates, suggesting stronger purchase intent.
Experiments 3 and 4: Version A outperforms B, indicating the tested change may reduce cart intent.
Experiment 5: Shows a notably large uplift in Version B, making it a strong candidate for further validation.
Overall: Mid-funnel behavior varies by experiment, reinforcing that UI changes impact intent differently across tests.
'''

# %%
# Completed Purchase Ratio - as in during this Experiment (at the version level - A or B)
# For this we will again create a new column

agg_df["Conversion_Rate(Cmpltd_Pur-Ratio)"] = agg_df["Total completed Purchases"]/agg_df["Total Sessions"]


# %%
agg_df.groupby(["experiment_id", "Experiment Version"])[["Conversion_Rate(Cmpltd_Pur-Ratio)"]].mean()

# %%
'''
FUNNEL-ANALYSIS (VERSION A | B) BREAKDOWN:
Click-through = interest
Add-to-cart = intent
Purchase = commitment
'''
#1. Click-through Behaviour 
#2. Total added to cart â†’ mid-funnel intent
#3. Completed Purchase Ratio - as in during this Experiment (at the version level - A or B)


# %%
agg_df.groupby(["experiment_id", "Experiment Version"]).apply(lambda x: pd.Series(
    {"Click-through_Behaviour": x["Total clickedProduct"].sum()/x["Total Sessions"].sum(),
     "Add_to_cart Rate": x["Total added to cart"].sum()/x["Total Sessions"].sum(),
     "Conversion_Rate(Cmpltd_Pur-Ratio)": x["Total completed Purchases"].sum()/x["Total Sessions"].sum()
}))

# %%
'''
FINDING AND INSIGHT 
---

### **1ï¸âƒ£ Click-through Behaviour (Top-of-Funnel)**

* This measures how often users click on products per session.
* Observations:

  * **Highest CTRs**: Experiment 4A (0.412) and Experiment 2B (0.407). Users are more engaged here.
  * **Lowest CTRs**: Experiment 5A (0.256) and Experiment 6A/B (~0.305), meaning these experiments got fewer clicks relative to sessions.
  * Some experiments show **version B performing better** (e.g., 1B vs 1A, 2B vs 2A), while others show A better (4A vs 4B).

**Insight:** Certain versions (like 2B and 4A) are better at attracting initial user interest.

---

### **2ï¸âƒ£ Add-to-Cart Rate (Mid-Funnel)**

* Measures how many clicks turn into adding products to cart.
* Observations:

  * **Highest Add-to-Cart Rates**: 4A (0.123), 1B (0.1), 2B (0.08). These are good at mid-funnel conversion.
  * **Lowest**: 5A (0.024), 3B (0.035), indicating weak mid-funnel performance.
  * Version B generally **outperforms A** in mid-funnel for many experiments (e.g., 1, 2, 5).

**Insight:** Version B seems more effective at nudging users to consider a purchase (add-to-cart), except for some experiments like 4.

---

### **3ï¸âƒ£ Completed Purchase Ratio (Bottom-of-Funnel / Conversion)**

* Measures actual purchase completions per session.
* Observations:

  * **Highest Conversion Ratios**: 7B (0.27), 4B (0.245), 3B (0.235) â€” these versions successfully convert mid-funnel activity into purchases.
  * **Lowest Conversion Ratios**: 8A (0.136), 3A (0.075) â€” even if CTR is decent, they fail to convert.
  * Version B generally **outperforms A** for the bottom-of-funnel metric (7B, 3B, 4B).

**Insight:** Version B tends to do better at actual conversion for most experiments, even if top-of-funnel clicks are sometimes higher in version A.

---

### **Overall Patterns**

1. **Top-of-Funnel vs Bottom-of-Funnel**:

   * High CTR does **not always** mean high conversion. E.g., 3A has decent clicks (0.349) but low purchases (0.075).
   * Some experiments have better **bottom-funnel performance** even with lower CTR, like 3B.

2. **Version A vs B**:

   * Version B often drives **higher add-to-cart and conversion rates**, even if version A has higher clicks.

3. **Key Experiments to Highlight for Optimization**:

   * **Experiment 2**: B version boosts both CTR and conversion.
   * **Experiment 4**: A version drives clicks, B drives conversions â†’ could combine insights.
   * **Experiment 7**: B version performs significantly better in conversion (0.27 vs 0.17).

---

If you want, I can **write a concise â€œInsights & Recommendationsâ€ section** suitable for your slides/README using only **1-page format**, summarizing which experiments & versions perform best and where to optimize.

Do you want me to do that next?

'''

# %%
# Click-through
agg_df["Click-through_Behaviour"] = agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total clickedProduct"].transform("sum") / agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total Sessions"].transform("sum")

# Add-to-cart
agg_df["Add_to_cart Rate"] = agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total added to cart"].transform("sum") / agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total Sessions"].transform("sum")

# Completed purchases
agg_df["Conversion_Rate(Cmpltd_Pur-Ratio)"] = agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total completed Purchases"].transform("sum") / agg_df.groupby(
    ["experiment_id", "Experiment Version"]
)["Total Sessions"].transform("sum")


# %%
agg_df

# %%
'''
 Highest lift experiments & versions

User Behavior
 Avg pages viewed per session (by experiment/version)
 Session frequency patterns vs conversion
 Correlation: engagement â†’ conversion

Customer Segmentation
 Country/city/signup-date patterns vs conversion
 Segment-specific experiment response
 High-value or discount-sensitive segments

Temporal Trends
 Conversion trends by signup/session date
 Early vs late exposure behavior

Event-Level Insights
 Sessions with clicks but no add-to-cart/purchase (%)
 Drop-off patterns (Click â†’ Add-to-cart â†’ Purchase) by experiment

Outlier Detection & Data Quality
 Extreme values in pages/orders/order value
 Inconsistent session data (purchase recorded but no order, etc.)
'''

# %%
behaviour = agg_df.groupby(["experiment_id", "Experiment Version"])[["Click-through_Behaviour"]]

behaviour.describe()

# %%
# USING .pivot() function in Python 
'''
Syntax and Parameters
The basic syntax is df.pivot(index=None, columns=None, values=None). 
index: The column(s) to use to make the new DataFrame's index (row labels). If not specified, the existing index is used.
columns: The column(s) to use to make the new DataFrame's columns. Each unique value in this column becomes a new column header.
values: The column(s) to populate the new frame's values (cell values). If not specified, all remaining columns will be used. 
'''

# %%
# SO rmemeber we want the difference between A and B for all 3 metrics --> click-through behaviour, add to cart actions, purchases completion

agg_df

# %%
click_TBeh = agg_df.pivot_table(index="experiment_id", columns="Experiment Version", values="Click-through_Behaviour") 

# %%
click_TBeh.describe()

# %%
'''
Key takeaways:
Mean
A mean CTR: 0.33798
B mean CTR: 0.34440
 On average, B performs slightly better than A on CTR across experiments.

Std (variation across experiments)
A: 0.04696
B: 0.04106
 CTR performance is fairly consistent across experiments for both versions.

Min / Max
Worst A CTR: 0.256 
Best A CTR: 0.412
Worst B CTR: 0.303
Best B CTR: 0.407
 Some experiments favor A, others favor B â†’ this is important.

What this answers vs what it does NOT
 This DOES answer:
â€œOverall, is B better than A on average for CTR?â€
â†’ Slightly yes
â€œHow variable is CTR performance across experiments?â€
â†’ Not wildly volatile

X This does NOT answer:
Which specific experiments had the biggest improvement?
How large is the improvement per experiment?
Is the lift meaningful or just noise?

For that, we must compute lift. '''

# %%
'''
Lift percentage measures the incremental impact or improvement of a change (like an ad campaign or new feature) 
compared to a baseline or control group, showing the percentage increase in desired actions (conversions, searches, etc.).
It Answers: "How much better did the new version perform than the old one?" by calculating 

Lift % FORMULA:
(New Rate - Old Rate) / Old Rate 

revealing the true effect beyond random chance or existing demand. 
'''

# %%
# SO Since we have the mean vlaues and also the min/max values for experiment A and B WE KNOW THIS:
# Worst A CTR: 0.256 | Best A CTR: 0.412 --> The best value for A here is obviously greater than B BUT the worst value for B is much lesser than A 
# Worst B CTR: 0.303 | Best B CTR: 0.407 
# Mean CTR: 0.33798 | B mean CTR: 0.34440 --> OVERALL (avg/mean wise) B performs better than A when it comes to Click-Through behaviour

click_TBeh["Click_TBeh_Lift%"] = ((click_TBeh["B"]  - click_TBeh["A"])/click_TBeh["A"])*100

# %%
click_TBeh

# %%
click_TBeh.sort_values("Click_TBeh_Lift%", ascending=False)

# %%
'''
Version B does not consistently outperform Version A across all experiments.
Although the overall mean CTR of Version B is slightly higher than Version A, lift analysis shows mixed results at the experiment level.

Only 3 out of 8 experiments show strong positive CTR lift for Version B.
Experiments 2 (+26.6%), 5 (+23.5%), and 1 (+11.9%) demonstrate meaningful improvements, indicating that Version B can significantly increase 
engagement in specific contexts.

Half of the experiments experienced neutral or negative lift.
Experiments 3, 4, 7, and 8 show CTR declines between âˆ’8% to âˆ’10%, suggesting that Version B may reduce engagement under certain conditions.

CTR performance is highly context-dependent rather than universally improved by Version B.
This indicates that experiment design, user segment, or traffic source likely influences whether Version B is effective.

Relying only on average CTR would have hidden these risks.
Lift-based comparison provides a more accurate evaluation of experiment impact and prevents misleading conclusions from aggregate metrics alone.
'''

# %%
agg_df

# %%
Add_To_C = agg_df.pivot_table(index="experiment_id", columns="Experiment Version", values="Add_to_cart Rate")

Add_To_C.describe()

# %%
'''
Lift % FORMULA:
((New Rate - Old Rate) / Old Rate) *100 '''

Add_To_C["Add_To_C_Lift%"] = ((Add_To_C["B"] - Add_To_C["A"])/Add_To_C["A"])*100

Add_To_C

# %%
agg_df

# %%
Cmplt_Pur = agg_df.pivot_table(index="experiment_id", columns="Experiment Version", values="Conversion_Rate(Cmpltd_Pur-Ratio)")

Cmplt_Pur.describe()

# %%
'''
Lift % FORMULA:
((New Rate - Old Rate) / Old Rate) *100 '''

Cmplt_Pur["Cmplt_Pur_Lift%"] = ((Cmplt_Pur["B"] - Cmplt_Pur["A"])/ Cmplt_Pur["A"])*100

Cmplt_Pur

# %%


# %%
''' 
Key Experiment Insights (Across Funnel):
Completed Purchase is the primary decision metric, as it directly reflects revenue impact.
Version B shows higher average completed purchase rates compared to Version A across experiments.
5 out of 8 experiments show strong positive lift in completed purchases, indicating B often converts better buyers.
Some experiments show negative CTR or Add-to-Cart lift but positive purchase lift, suggesting quality of traffic improved, not just volume.
Click behavior alone is insufficient â€” several experiments with lower CTR still resulted in higher final conversions.
Add-to-Cart lift acts as a mid-funnel diagnostic, helping explain why certain experiments succeed or fail.
Overall, Version B demonstrates stronger bottom-line performance, even when early-funnel metrics are mixed.
'''

# %%
# SO WE HAD all of this BELOW - BUT did not fit the TEST narrative - so decided on only 2 important ones 
'''
 Highest lift experiments & versions

User Behavior
 Avg pages viewed per session (by experiment/version)
 Session frequency patterns vs conversion
 Correlation: engagement â†’ conversion

Customer Segmentation
 Country/city/signup-date patterns vs conversion
 Segment-specific experiment response
 High-value or discount-sensitive segments

Temporal Trends
 Conversion trends by signup/session date
 Early vs late exposure behavior

Event-Level Insights
 Sessions with clicks but no add-to-cart/purchase (%)
 Drop-off patterns (Click â†’ Add-to-cart â†’ Purchase) by experiment

Outlier Detection & Data Quality
 Extreme values in pages/orders/order value
 Inconsistent session data (purchase recorded but no order, etc.)
'''

# %%
'''
EDA 5 â€” Funnel Drop-Off Analysis

(Answer these, then STOP)

Funnel Entry
How many sessions enter the funnel in Control vs Variant?
Does Variant increase or decrease funnel entry rate?

Step-by-Step Drop-Off
What % of users go from Session â†’ Product View (A vs B)?
What % of users go from Product View â†’ Add to Cart (A vs B)?
What % of users go from Add to Cart â†’ Checkout (A vs B)?
What % of users go from Checkout â†’ Purchase (A vs B)?

Drop-Off Comparison
At which step is the largest drop-off overall?
At which step is the largest difference between A and B?
Is Variant B better, worse, or neutral at each step?

Funnel Outcome
What % of sessions end in a purchase (A vs B)?
Is overall funnel completion higher in Variant B?

Sanity Checks
Does Variant B improve early steps but hurt later steps?
Are improvements consistent across the full funnel?

 Stop here. Thatâ€™s EDA 5.
'''

# %%
'''
Funnel Entry
How many sessions enter the funnel in Control vs Variant?
Does Variant increase or decrease funnel entry rate?
'''

# %%
'''
EDA 5 â€” Funnel Drop-Off
(Where do people quit?)

Imagine someone comes to the website.

Weâ€™re asking:
How many people came to the site?
How many actually looked at a product?
How many added something to cart?
How many went to checkout?
How many actually bought?

Now compare Group A vs Group B.
Thatâ€™s it.

What are we REALLY looking for?
At which step do most people leave?
Does Group B lose fewer people at any step?

Does Group B:

Help people add to cart?
Help people finish checkout?
Or does Group B help early but mess things up later?

Example (plain English):
â€œMore people in B add items to cart, but many of them quit at checkout.â€

Thatâ€™s a real finding.
'''

# %%
'''
EDA 5 â€” Funnel Drop-Off
(Where do people quit?)

Imagine someone comes to the website.

Weâ€™re asking:
How many people came to the site?
How many actually looked at a product?
How many added something to cart?
How many went to checkout?
How many actually bought?
'''

# %%
Came_to_site = agg_df[agg_df["Total Pages Viewed"] > 0]

visited_site = (Came_to_site.groupby(["experiment_id", "element_tested", "Experiment Version"])["user_id"].nunique().reset_index(name="users_count"))
# NOT .count() ---> .nunique()

visited_site

# %%
# What does this ---> .reset_index(name="users_count") ---> line do? and what is it important for?

'''Why this line is IMPORTANT

Without it:
Column has no name
Merging later becomes painful
README tables look messy
Plots break

With it:
Clean dataframe
Easy merges
Clear meaning '''

# %%
'''EDA 5 â€” Funnel Step 1: Site Visit

(Users who came to the site, Total Pages Viewed > 0)

Key Insights
All experiments have a good baseline of site visitors, ranging roughly 82â€“115 users per variant.
Site visit counts differ slightly between Control (A) and Variant (B):
Some experiments have more visitors in A (e.g., UrgencyBanner, Exp 1 â†’ 115 vs 88)
Others have more in B (e.g., PriceCharm, Exp 2 â†’ 112 vs 83)
Differences reflect traffic allocation, not experiment impact.
This step establishes the denominator for all downstream funnel metrics (product view, add-to-cart, purchase).

Interpretation Notes
Site visits do not measure conversion or engagement beyond arrival.
Differences in visitor counts are normal; focus is on later funnel steps for performance evaluation.

Methodology (1-liner for README)
A site visit is defined as any user with Total Pages Viewed > 0, as there is no explicit session_start event in the dataset.'''

# %%
agg_df.columns

# %%
pd.reset_option('display.max_columns')

# %%
agg_df.columns

# %%
agg_df["Total clickedProduct"]

# %% [markdown]
# 

# %%
agg_df["Total clickedProduct"].describe()

# %%
# How many actually looked at a product?
click_OnProd = agg_df[agg_df["Total clickedProduct"] > 0]

Lookd_Prod = (click_OnProd.groupby(["experiment_id", "element_tested", "Experiment Version"])["user_id"].nunique().reset_index(name="user_count"))

Lookd_Prod

# %%
'''
EDA 5 â€” Funnel Step 2: Product View
(Users who clicked at least one product)

Key Insights
Only a subset of site visitors actually click to view a product.
Example: In Experiment 1 (UrgencyBanner), Version A â†’ 39 users, Version B â†’ 34 users.

Variant effects vary by experiment:
Some Variants (like PriceCharm B) increase product views compared to Control.
Others (like TrustBadge B) decrease product views compared to Control.
Step 2 shows early engagement differences, before cart or purchase.
These numbers will serve as the denominator for the next funnel step: Add to Cart.

Interpretation Notes
Product view = interest/engagement, not purchase intent.
Higher or lower counts here indicate whether the experiment attracts attention, not whether it converts.

Methodology
Product view is defined as any user with Total clickedProduct > 0.
'''

# %%
'''
Weâ€™re asking:
How many people came to the site? DONE
How many actually looked at a product? DONE
How many added something to cart?
How many went to checkout?
How many actually bought? 
'''

# %%
# How many added something to cart?

agg_df.columns # IMPORTANT column ---> 'Total added to cart'

# %%
# How many added something to cart?
agg_df["Total added to cart"].describe() # SO the max is 1 - so jsut in case anything greater than 0 

# %%
# How many added something to cart?
added_toC = agg_df[agg_df["Total added to cart"] > 0]

total_add_toC = (added_toC.groupby(["experiment_id", "element_tested", "Experiment Version"])["user_id"].nunique().reset_index(name="user_COUNT"))
total_add_toC

# %%
'''
EDA 5 â€” Funnel Step: Add to Cart
(Users who added at least one item to cart)

What this step means (plain English)
These users liked a product enough to click â€œAdd to Cartâ€
This is the first real buying signal
Big drop here = people are browsing but not committing

Key Insights
Add-to-cart numbers are very low across all experiments
Mostly between 2â€“14 users per variant
This is expected but highlights a major drop-off from site visits

Variant B improves add-to-cart in some experiments:
Exp 1 â€“ UrgencyBanner: B (9) > A (8)
Exp 2 â€“ PriceCharm: B (9) > A (6)
Exp 5 â€“ FreeShippingBadge: B (7) > A (2) â†’ strong relative lift
Exp 7 â€“ FreeShippingBadge: B (5) > A (4)

Variant A performs better in a few cases:
Exp 3 â€“ TrustBadge: A (5) > B (3)
Exp 4 â€“ UrgencyBanner: A (14) > B (11)
Some experiments show no difference
Exp 6 â€“ PriceCharm: A = B (6)
Exp 8 â€“ TrustBadge: A = B (10)

Big Picture Takeaway
Most users never reach add-to-cart, even after visiting the site.
FreeShippingBadge (Variant B) shows the clearest improvement at this stage.
Effects are directional, not final conclusions â€” sample sizes are small.

Methodology (1 line for README)
Add to cart is defined as users with Total added to cart > 0, counted as unique users per experiment variant. '''

# %%
agg_df.columns

# %%
'''How many went to checkout? -- NO COLUMN related to this 

Checkout Step Limitation
The dataset does not contain an explicit checkout-start event (e.g., checkout page views or payment initiation). 
As a result, checkout behavior could not be analyzed as a separate funnel step. 
The funnel therefore progresses directly from Add to Cart to Completed Purchase.
'''

# %%
# How many actually bought? 

Total_ComplPur = agg_df[agg_df["Total completed Purchases"] > 0]

Total_Bought = Total_ComplPur.groupby(["experiment_id", "element_tested", "Experiment Version"])["user_id"].nunique().reset_index(name="users_COUNT")

Total_Bought

# %%
# INSIGHT FOR FINAL EDA 5

'''
Perfect â€” this is **exactly** the final funnel table we needed.
Now Iâ€™ll give you **clear, README-ready insights** for **â€œHow many actually bought?â€** based *only* on this output (no assumptions, no fluff).

---

## ðŸ”š Funnel Step: Purchase Completion â€” Key Insights

### Overall pattern

* **Purchase behavior meaningfully differs between Version A and B across experiments**, 
indicating that UI elements can strongly influence final conversion â€” not just clicks.

---

## ðŸ§ª Experiment-wise Insights

### **Experiment 1 â€” UrgencyBanner**

* **Version A outperformed B** (23 vs 16 buyers)
* Indicates urgency messaging here may **work better in its original form**
* Suggests Version B possibly introduced friction or reduced trust

---

### **Experiment 2 â€” PriceCharm**

* **Version B massively outperformed A** (24 vs 12 buyers)
* One of the **strongest positive uplifts** across all experiments
* Clear signal that **pricing presentation significantly impacts purchase decisions**

ðŸ“Œ *This is a high-confidence winner candidate*

---

### **Experiment 3 â€” TrustBadge**

* **Version B strongly outperformed A** (20 vs 8 buyers)
* Nearly **2.5Ã— more buyers** in B
* Suggests redesigned or repositioned trust signals meaningfully reduce purchase hesitation

ðŸ“Œ *Trust elements matter most at checkout-decision stage*

---

### **Experiment 4 â€” UrgencyBanner**

* **Version B outperformed A** (26 vs 17 buyers)
* Opposite outcome vs Experiment 1
* Indicates **context or implementation matters more than the element itself**

ðŸ“Œ *Urgency is not universally positive â€” execution matters*

---

### **Experiment 5 â€” FreeShippingBadge**

* **Version A slightly outperformed B** (15 vs 12 buyers)
* Indicates B version may not have communicated value clearly
* Free shipping alone may not be a strong purchase driver here

---

### **Experiment 6 â€” PriceCharm**

* **Version A outperformed B** (20 vs 14 buyers)
* Contrasts with Experiment 2
* Suggests **pricing charm effectiveness varies by context, audience, or product mix**

---

### **Experiment 7 â€” FreeShippingBadge**

* **Version B significantly outperformed A** (24 vs 16 buyers)
* Indicates **badge placement or wording can materially impact checkout behavior**

---

### **Experiment 8 â€” TrustBadge**

* **Version B outperformed A** (21 vs 14 buyers)
* Consistent with Experiment 3
* Reinforces that **trust reinforcement near purchase is highly effective**

---

## ðŸ“Œ Cross-Experiment Takeaways (README gold)

* **TrustBadge (Version B)** shows **consistent positive impact** on purchase completion
* **PriceCharm results are mixed**, suggesting sensitivity to implementation details
* **Urgency elements show inconsistent results**, indicating potential overuse or trust trade-offs
* **FreeShippingBadge effectiveness depends on presentation**, not presence alone

---

## What this sets up next (donâ€™t do yet)

Next step is **conversion rates**, not raw counts:

* Add-to-Cart â†’ Purchase conversion
* Visited â†’ Purchase conversion

Thatâ€™s where we quantify **real lift** and decide winners statistically.

---

Youâ€™re officially done with **funnel counts** 
Ping me when youâ€™re ready to **merge all funnel steps into one table** or move to **EDA 6 / statistical testing**.

'''

# %%
# WHY was Completed Purchase much higher for each expeirment - THAN the Add to Cart? 
# Like HOW is it possible to complete purchase without adding to cart?

# %%
'''
Youâ€™re **NOT tripping**.
Your instinct is 100% correct â€” *in a clean, real-world funnel*, this **should not happen**.

Letâ€™s break it down **very simply**, no business jargon.

---

## What *should* logically happen

Normally, the order is:

```
Visit site
â†’ View product
â†’ Add to cart
â†’ Checkout
â†’ Purchase
```

So logically:

> **#Add-to-Cart users â‰¥ #Purchase users**

But in **your data**, youâ€™re seeing:

> **Purchases > Add-to-Cart**

That means the dataset **does NOT track the funnel cleanly**.

---

## Why this happens (VERY common in simulated / event-based data)

###  Reason 1 (Most likely): **Add-to-cart is under-tracked**

Some purchases are logged **without logging the add-to-cart event**.

Example:

* User buys directly from:

  * â€œBuy Nowâ€ button
  * Quick checkout
  * Saved cart from previous session
  * One-click purchase

 Purchase event fires
 Add-to-cart event never fires

---

###  Reason 2: **Aggregated user-level data hides sequence**

Your table is **user-level totals**, not event order.

Example:

* User added to cart **last week**
* Purchased **this week**
* Your aggregation shows:

  * `Total added to cart = 0`
  * `Total completed purchases = 1`

 Funnel order looks broken even though it wasnâ€™t

---

###  Reason 3: **Session mismatch**

* Add-to-cart may have happened in a session **outside the experiment window**
* Purchase happened **inside** the window

 Only purchase shows up

---

###  What it is NOT

*  You did NOT code anything wrong
*  `.nunique()` is correct
*  Your logic is sound
*  This is NOT a Pandas issue

This is a **data-generation / logging issue**, not an analysis mistake.

---

## How to explain this cleanly in your README (copy-paste safe)

> *â€œDue to event-level limitations in the dataset, some users record completed purchases without a corresponding add-to-cart event. This can occur due to direct checkout flows, prior-session cart persistence, or incomplete event tracking. As a result, funnel steps are treated as independent behavioral signals rather than a strict sequential path.â€*

That sentence alone = **senior-level awareness**.

---

## What WE do going forward (important)

For this project:

* Treat funnel steps as **independent engagement indicators**
* **Do NOT force** add-to-cart â†’ purchase dependency
* Conversion rate =
  **Purchases / Visitors**, not Purchases / Add-to-Cart

This keeps your analysis **honest and defensible**.

---
'''

# %%
'''
So WHY do we still do .merge()?
Because groupby â‰  combine tables.

Think of it like this (real-world analogy):
You have 4 Excel sheets:
Sheet 1 â†’ â€œUsers who visitedâ€
Sheet 2 â†’ â€œUsers who viewed productâ€
Sheet 3 â†’ â€œUsers who added to cartâ€
Sheet 4 â†’ â€œUsers who purchasedâ€

Each sheet is already summarized by:
experiment | element | version | number

Now you want ONE sheet that looks like:
experiment | element | version | visited | viewed | added | purchased


--> merge() is just telling pandas:

â€œMatch rows that belong to the same experiment + element + version,
and put their numbers side-by-side.â€
Thatâ€™s all it does. 
'''

# %%
# SO WE'RE gonna use .merege to merge our 4 tables (earlier created using .groupby()) 
# THSES 4 TABLES: visited_site | Lookd_Prod | total_add_toC | Total_Bought

visited_site  # How many Visited the WebSite - As in logged in basically 
Lookd_Prod   # How many looked at some Products - Showed some interest at least 
total_add_toC # How many actually Added Products to their Cart - Showed Intent of Buying
Total_Bought # How many actually Completed Purchase - CONVERTED Fully - Acted on their Intent, Were Serious Enough, Were  

# %%
Total_Bought

# %%
'''
EDA 5 â€” Funnel Drop-Off
(Where do people quit?)'''

# %%
Funnel_Drop_off = visited_site.merge(Lookd_Prod,
                                      how='left', on=['experiment_id', 'element_tested', 'Experiment Version']).merge(total_add_toC, 
                                      how='left', on=['experiment_id', 'element_tested', 'Experiment Version']).merge(Total_Bought, 
                                      how='left', on=['experiment_id', 'element_tested', 'Experiment Version']).rename(
                                          columns={
                                            "users_count": "Visited_Site_Users",
                                            "user_count" : "Viewed_Product_Users",
                                            "user_COUNT" : "Added_to_Cart_Users",
                                            "users_COUNT" : "Purchased_Users"
                                          }).fillna(0) # Just in case some NA values show up in the final merged table

# %%
Funnel_Drop_off

# %%
# How effective is the landing experience at generating product interest?
Funnel_Drop_off["Visit_to_ViewdProd_Rate"] = (Funnel_Drop_off["Viewed_Product_Users"]/Funnel_Drop_off["Visited_Site_Users"])

# Once users are interested, how many develop purchase intent?
Funnel_Drop_off["ViewProd_to_AddCart_Rate"] = (Funnel_Drop_off["Added_to_Cart_Users"]/Funnel_Drop_off["Viewed_Product_Users"])

# How smooth is the checkout decision once intent exists?
Funnel_Drop_off["AddCart_to_Purchase_Rate"] = (Funnel_Drop_off["Purchased_Users"]/Funnel_Drop_off["Added_to_Cart_Users"])

# Which variant actually converts better overall?
Funnel_Drop_off["Visit_to_Purchase_Rate"] = (Funnel_Drop_off["Purchased_Users"]/Funnel_Drop_off["Visited_Site_Users"])

# Note: Metrics were calculated as ratios and formatted as percentages at the visualization layer for clarity.

Funnel_Drop_off

# %%
'''
INSIGHTS: 

Perfect â€” this is exactly the right moment to lock **EDA #5 README-ready insights**.
Iâ€™ll keep this **simple, professional, senior-level**, and aligned with your earlier assumption note.

---

## EDA #5 â€” Funnel Drop-Off Analysis (README Insights)

### Key Observations

* **Product interest varies meaningfully by experiment variant.**
  Visit â†’ Product View rates generally fall between **~25% and ~42%**, indicating that landing experiences influence whether users engage with products after arriving on the site.

* **The largest drop-off consistently occurs between product view and add-to-cart.**
  View â†’ Add-to-Cart rates are relatively low across most experiments (often **10â€“30%**), suggesting friction in moving users from interest to purchase intent.

* **Purchase behavior does not strictly follow a linear funnel.**
  In several experiments, the Add-to-Cart â†’ Purchase rate exceeds 100%, indicating that some users complete purchases without a recorded add-to-cart event.
  This reinforces that funnel stages should be interpreted as **engagement signals**, not mandatory sequential steps.

* **Overall conversion performance differs clearly between variants.**
  Visit â†’ Purchase rates range from **~8% to ~27%**, showing meaningful differences in how effectively variants drive end-to-end conversion.

* **Some variants outperform earlier in the funnel, others later.**
  Certain variants generate stronger product interest (higher Visit â†’ View), while others show stronger final conversion (higher Visit â†’ Purchase), highlighting that optimization opportunities differ by funnel stage.

---

### Interpretation Notes

* Due to the **simulated and aggregated nature of the dataset**, funnel metrics should not be interpreted as strict checkout steps.
* Users may purchase via direct flows (e.g., â€œBuy Nowâ€), strong pre-existing intent, or paths not fully captured by add-to-cart events.
* As a result, funnel rates are best used to **compare relative performance between variants**, rather than to enforce real-world behavioral assumptions.

---

### Why These 4 Funnel Metrics Matter

These four rates answer four **business-critical questions**:

1. **Visit â†’ View**: How effective is the landing experience at generating product interest?
2. **View â†’ Cart**: How well does interest convert into purchase intent?
3. **Cart â†’ Purchase**: How smooth is the final decision stage once intent exists?
4. **Visit â†’ Purchase**: Which variant actually converts best overall?

Together, they provide a **complete, decision-oriented view** of where each experiment variant succeeds or loses users.

---

If you want, next we can:

* Condense this into **3 ultra-tight bullets for the Executive Deck**, or
* Move straight into **EDA #6 â€” Segment-Level Experiment Performance** (this is where the project really starts to feel ðŸ”¥).
'''


# %%
agg_df.columns

# %%
agg_df

# %%
'''
1. UrgencyBanner
What it is:
A visual message that creates time or scarcity pressure.
Examples youâ€™ve 100% seen:
â€œ Only 3 left in stockâ€
â€œ Sale ends in 2 hoursâ€
â€œ12 people are viewing this right nowâ€

2. PriceCharm
What it is:
A pricing tactic that makes prices feel cheaper.
Classic examples:
$9.99 instead of $10
$49.95
â€œFrom $19â€ instead of â€œ$19.00â€

3. FreeShippingBadge
What it is:
A badge or message highlighting free shipping.
Examples:
â€œFree Shippingâ€
â€œFree shipping on orders over $50â€
â€œNo delivery feesâ€

4. TrustBadge
What it is:
Visual elements that increase credibility & safety perception.
Examples:
â€œSecure Checkoutâ€
â€œTrusted by 1M+ customersâ€
Payment logos (Visa, PayPal)
Money-back guarantee badges '''

# %%
# EDA 6 Segment-Level Experiment Performance

# %%
'''
What does â€œSegment-Level Experiment Performanceâ€ even mean?
Plain English
Does the experiment work better for some types of users than others? 

Step 2: What counts as a â€œsegmentâ€?
A segment is any meaningful way to group users so that:
Behavior differs across groups
The experiment might affect them differently
The insight is actionable

Industry rule of thumb:
If a segment doesnâ€™t change decisions, donâ€™t segment on it.'''


# %%
# labels= ---> RULE labels are always in an ascending manner ---> Lowest to Highest

agg_df["Engagement_Level"] = pd.qcut(agg_df["Total Pages Viewed"], q=3, labels= ["Low Engagement", "Medium Engagement", "High Engagement"])


# %%


# %%
agg_df.groupby(["Engagement_Level", "Experiment Version"])[["Conversion_Rate(Cmpltd_Pur-Ratio)"]].mean()

# %%


# %%


# %%
agg_df["Engagement_Level"].value_counts()

# %%
'''
(Low - Medium - High Engagement Levels)
EDA #6 â€” Segment-Level Experiment Performance (Engagement Level)
- Users were segmented into Low, Medium, and High Engagement groups based on total pages viewed to understand 
  how engagement depth impacts experiment performance.

- Experiment Version B outperformed Version A across all engagement segments, indicating a consistent and 
  robust uplift rather than a segment-specific effect.

- The largest conversion lift was observed among Low and Medium Engagement users, 
  suggesting Version B is especially effective at converting users with limited browsing behavior.

- High Engagement users showed strong conversion rates in both variants, 
  but Version B still delivered a measurable improvement.

 - Overall, Version B reduces reliance on deep user exploration by enabling faster 
   and more confident purchase decisions across engagement levels.
'''

# %%
agg_df.columns

# %%
# SO the 3 columns we'll be using are ---> 'Total clickedProduct', 'Total added to cart', 'Add_to_cart Rate'

# %%
agg_df["Total clickedProduct"].describe()

# %%
'''
â€œPurchase intent was derived using quantile-based rankings across multiple pre-purchase signals 
(product clicks, add-to-cart actions, and add-to-cart rate) to avoid scale bias between raw counts and ratios.â€
'''

# %%
'''
FINAL SEGMENTATION DESIGN (LOCKED)

Segment 1 â€” Engagement Level
Signals (interest / exploration):
Total Pages Viewed
Total clickedProduct

Segment 2 â€” Purchase Intent Level
Signals (buying seriousness):
Total added to cart
Add_to_cart Rate

Outcome metric (for comparison, not segmentation):
Conversion_Rate(Cmpltd_Pur-Ratio) â†’ weâ€™ll take the mean, same as before.
'''

# %%
'''
logic:
High + High â†’ High
Low + Low â†’ Low
Else â†’ Medium
'''

# %%
# COLUMNS WE WILL BE USING:

#   ENGAGEMENT level                  |  Purchase INTENT                    |  Purchase Conversion
# "Total Pages Viewed"                | "Total added to cart"               | "Conversion_Rate(Cmpltd_Pur-Ratio)"   
# "Total clickedProduct"              | "Add_to_cart Rate"                  |
# "Conversion_Rate(Cmpltd_Pur-Ratio)" | "Conversion_Rate(Cmpltd_Pur-Ratio)" | 


# %%
agg_df["Total clickedProduct"].describe()

# %%
# ENGAGEMENT LEVEL 

agg_df["PagesView_Level"] = pd.qcut(agg_df["Total Pages Viewed"], q=3, labels=["Low", "Medium", "High"])


# %%
agg_df["ClickProd-Level"] = "No Clicks"

# %%
mask = agg_df["Total clickedProduct"] > 0

# %%
agg_df["Total clickedProduct"].describe() # SO There's clearly only 3 valeus ---> 0 1 2 

# %%
agg_df.loc[agg_df["Total clickedProduct"] == 0, "ClickProd-Level"] = "Low"

agg_df.loc[agg_df["Total clickedProduct"] == 1, "ClickProd-Level"] = "Medium"

agg_df.loc[agg_df["Total clickedProduct"] == 2, "ClickProd-Level"] = "High"

# %%
def combined_engagement(row):
    # If either metric is High â†’ High
    if row["PagesView_Level"] == "High" or row["ClickProd-Level"] == "High":
        return "High"
    
    # If both metrics are Low â†’ Low
    elif row["PagesView_Level"] == "Low" and row["ClickProd-Level"] == "Low":
        return "Low"
    
    # If one metric is Medium and the other is Low â†’ Medium
    elif (row["PagesView_Level"] == "Medium" and row["ClickProd-Level"] == "Low") \
         or (row["PagesView_Level"] == "Low" and row["ClickProd-Level"] == "Medium"):
        return "Medium"
    
    # If both metrics are Medium â†’ Medium (just reinforcing)
    elif row["PagesView_Level"] == "Medium" and row["ClickProd-Level"] == "Medium":
        return "Medium"
    
    # Fallback just in case
    else:
        return "Medium"

agg_df["Engagement_Level"] = agg_df.apply(combined_engagement, axis=1)


# %%
agg_df["Engagement_Level"].value_counts()

# %%
agg_df["Total added to cart"].describe()

# %%
agg_df["Add_to_cart Rate"].describe()

# %%
# SO the most anyone's added is just 1 and the upper quartile of add to cart rate is more than anything more than 50% ---> 0.071429
# SO WE DO  "Add_to_cart Rate" "Total added to cart"

def purchase_intent(row):

    if row["Total added to cart"] == 1 and row["Add_to_cart Rate"] > 0.071429:
        return "High"
    
    elif row["Total added to cart"] == 0:
        return "Low"
    
    else:
        return "Medium"


agg_df["Purchase_Intent_Level"] = agg_df.apply(purchase_intent, axis= 1)

# %%
agg_df["Purchase_Intent_Level"].value_counts()

# %%
Final_Conversion_Table = (agg_df.groupby(["Engagement_Level", "Purchase_Intent_Level", "Experiment Version", "element_tested"], as_index=False).agg(
    avg_Converion = ("Conversion_Rate(Cmpltd_Pur-Ratio)", "mean"), Users = ("user_id", "nunique" )))

Final_Conversion_Table

# %%
agg_df

# %%
from sqlalchemy import create_engine

# CONNECTING to our DB 

DB_USER = "postgres"
DB_PASS = "ThePasswordis19!"
DB_HOST = "localhost"
DB_PORT = 5432
DB_NAME = "E-Commerce_Project"
SCHEMA = "ecom"

# Lets create engine 

engine = create_engine(f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}")


# %%
TABLE_NAME = "AggEtable_analysisLvL"
SCHEMA = "ecom"

agg_df.to_sql(
    name=TABLE_NAME,
    con=engine,
    schema=SCHEMA,
    if_exists="replace",   
    index=False,
    method="multi"
)

print("OKAY, DONE!")

# %%
analy
